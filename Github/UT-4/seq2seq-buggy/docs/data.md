## Ready-to-use Datasets

We prepared a few datasets to help you get started. We also provide data generation scripts that
make explicit what kind of processing has been done on the data.

| Dataset | Description | Training/Dev/Test Size | Vocabulary | Download |
| --- | --- | --- | --- | --- |
| WMT'16 EN-DE | Data for the [WMT'16 Translation Task](http://www.statmt.org/wmt16/translation-task.html) English to German. Training data is combined from Europarl v7, Common Crawl, and News Commentary v11. Development data sets include `newstest[2010-2015]`. `newstest2016` should serve as test data. All SGM files were converted to plain text.  | 4.56M/3K/2.6K | 50k Words <br/> 50k BPE| Download <br/> [Generate](https://github.com/dennybritz/seq2seq/blob/master/bin/data/wmt16_en_de.sh) |
| Toy Copy | A toy dataset where the target sequence is equal to the source sequence. The model must learn to copy the source sequence. | 10k/1k/1k | 20 | Download <br/> [Generate](https://github.com/dennybritz/seq2seq/blob/master/bin/data/toy.sh) |
| Toy Reverse | A toy dataset where the target sequence is equal to the reversed source sequence. The model must learn to reverse the source sequence. | 10k/1k/1k | 20 | Download <br/> [Generate](https://github.com/dennybritz/seq2seq/blob/master/bin/data/toy.sh) |

## Creating your own data

To use your own data you must bring it into the right format. A typical data preprocessing pipeline looks as follows:

1. Generate data in parallel text format
2. Tokenize your data
3. Create fixed vocabularies for your source and target data
4. (Optional) Use Subword Units to handle rare or unknown words

### Parallel Text Format

The input pipeline expects parallel `sources` and `targets` files where each pair of lines corresponds to one input/output example. The files must be in raw text format and tokenized with space delimiters. For example, an (extremely short and silly) Machine Translation dataset could consist of:

`sources`
```bash
I love cats .
Who are you ?
```

`targets`
```bash
J` aime les chats .
Qui es tu ?
```

### Tokenization

For good results, it is crucial to [tokenize your text](http://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html). Many tools to do tokenization are available, e.g.

- The Moses [`tokenizer.perl`](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl) script.
- Libraries such a [spaCy](https://spacy.io/docs/usage/processing-text), [nltk](http://www.nltk.org/api/nltk.tokenize.html) or [Stanford Tokenizer](http://nlp.stanford.edu/software/tokenizer.shtml).

For example, to use the Moses tokenizer:

```bash
# Clone from Github
git clone https://github.com/moses-smt/mosesdecoder.git

# Tokenize English (en) data
mosesdecoder/scripts/tokenizer/tokenizer.perl -l en -threads 8 < english_data > english_data.tok

# Tokenize German (de) data
mosesdecoder/scripts/tokenizer/tokenizer.perl -l de -threads 8 < german_data > german_data.tok
```

### Generating Vocabulary

A vocabulary file is a raw text file that contains one word per line. The total number of lines is equal to the size of the vocabulary and each token is mapped to its line number. The special tokens `UNK`, `SEQUENCE_START` and `SEQUENCE_END` are generated by the model and are not included in the vocabulary file. Their corresponding vocabulary ids are `vocab_size + 1`, `vocab_size + 2`, and `vocab_size + 3`, respectively.

We provide a helper script [`bin/tools/generate_vocab.py`](https://github.com/dennybritz/seq2seq/blob/master/bin/tools/generate_vocab.py) that takes in a raw text file of space-delimited tokens and generates a vocabulary file:

```shell
./bin/tools/generate_vocab.py \
  --input_file /data/source.txt \
  --output_file /data/source_vocab \
  --min_frequency 1 \
  --max_vocab_size 50000
```

### Subword Units (BPE)

In order to deal with an open vocabulary, rare words can be split into subword units as proposed in [Neural Machine Translation of Rare Words with Subword Units](http://arxiv.org/abs/1508.07909). This improves the model's translation performance particularly on rare words. Subword units are calculated using Byte Pair Encoding (BPE), which iteratively replaces the most frequent pair of symbols with a new symbol. The final symbol vocabulary is equal to the size of an initial vocabulary, all characters appearing in the text, plus the number of merge operations, which is a hyperparameter of the method. To apply BPE as a pre-processing step to your raw (tokenized) text input, follow the instructions below:

```bash
# Clone from Github
git clone https://github.com/rsennrich/subword-nmt
cd subword-nmt

# Learn a vocabulary using 10,000 merge operations
./learn_bpe.py -s 10000 < train.tok > codes.bpe
# Apply the vocabulary to the training file
./apply_bpe.py -c codes.bpe < train.tok > train.tok.bpe
```

The resulting BPE-processed files can be used as-is in place of the raw text files for training the NMT model. Note that you must now use the BPE vocabulary as your vocabulary file. You can do this by generating a vocabulary based on the BPE-processed files.

In the BPE-processed file the original words are split using a special `"@@ "` string. To recover the original tokenization you can simply perform a `sed "s/@@ //g"` operation on the BPE files and the model output. For more details, refer to the paper and [subword-nmt](https://github.com/rsennrich/subword-nmt) Github repository.


### Character Vocabulary

Sometimes you want to run training on characters instead of words or subword units. The [`bin/tools/generate_char_vocab.py`](https://github.com/dennybritz/seq2seq/blob/master/bin/tools/generate_char_vocab.py) can generate a vocabulary file that contains the unique set of characters found in the text:

```shell
./bin/tools/generate_char_vocab.py \
  < /data/source.txt \
  > /data/source_vocab.char.txt
```

To run training on characters you must pass the `--delimiter=""` flag to the training script to avoid splitting words on spaces. See the [Training documentation](training.md) for more details.

