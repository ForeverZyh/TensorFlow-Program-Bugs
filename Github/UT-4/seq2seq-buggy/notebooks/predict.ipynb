{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from seq2seq import models\n",
    "from seq2seq.data import data_utils, vocab\n",
    "from seq2seq.training import utils as training_utils\n",
    "from seq2seq.training import HParamsParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SOURCE = \"/tmp/toydata/train/vocab.txt\"\n",
    "VOCAB_TARGET = \"/tmp/toydata/train/vocab.txt\"\n",
    "MODEL = \"AttentionSeq2Seq\"\n",
    "MODEL_DIR = \"/Users/dennybritz/Downloads/toy\"\n",
    "INPUT_FILE = \"/tmp/toydata/test/sources.txt\"\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "source_vocab_info = vocab.get_vocab_info(VOCAB_SOURCE)\n",
    "target_vocab_info = vocab.get_vocab_info(VOCAB_TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find model class\n",
    "model_class = getattr(models, MODEL)\n",
    "\n",
    "# Parse parameter and merge with defaults\n",
    "hparams = model_class.default_params()\n",
    "hparams_parser = HParamsParser(hparams)\n",
    "saved_hparams = training_utils.read_hparams(os.path.join(MODEL_DIR, \"hparams.txt\"))\n",
    "hparams = hparams_parser.parse(saved_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = model_class(\n",
    "    source_vocab_info=source_vocab_info,\n",
    "    target_vocab_info=target_vocab_info,\n",
    "    params=hparams)\n",
    "featurizer = model.create_featurizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", 'tf_random_seed': None, '_task_id': 0, 'save_checkpoints_steps': None, '_is_chief': True, 'keep_checkpoint_every_n_hours': 10000, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x111828fd0>, 'keep_checkpoint_max': 5, 'save_summary_steps': 100, '_evaluation_master': '', '_environment': 'local', 'save_checkpoints_secs': 600, '_master': '', '_task_type': None, '_num_ps_replicas': 0}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x111831ae8>) includes params argument, but params are not passed to Estimator.\n"
     ]
    }
   ],
   "source": [
    "def model_fn(features, labels, params, mode):\n",
    "  \"\"\"Builds the model graph\"\"\"\n",
    "  return model(features, labels, params, mode)\n",
    "  \n",
    "# Create estimator\n",
    "estimator = tf.contrib.learn.estimator.Estimator(model_fn=model_fn, model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create input function for input file\n",
    "data_provider = lambda: data_utils.make_parallel_data_provider(\n",
    "        [INPUT_FILE], [INPUT_FILE], shuffle=False)\n",
    "input_fn = training_utils.create_input_fn(\n",
    "      data_provider,\n",
    "      featurizer,\n",
    "      BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating vocabulary lookup table of size 20\n",
      "INFO:tensorflow:Creating vocabulary lookup table of size 20\n",
      "Tensor(\"model/att_seq2seq/embedding_lookup_1:0\", shape=(32, 100), dtype=float32)\n",
      "Tensor(\"model/att_seq2seq/attention_decoder_1/RNN/while/rnn_step/fixed_decoder_inputs/embedding_lookup:0\", shape=(32, 100), dtype=float32)\n",
      "INFO:tensorflow:Loading model from checkpoint: /Users/dennybritz/Downloads/toy/model.ckpt-1-?????-of-00001.\n"
     ]
    }
   ],
   "source": [
    "predictions = estimator.predict(input_fn=input_fn)\n",
    "predictions_ = next(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7 7 13 12 12 12 5 12 4 4 4 4 12 12 12'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(VOCAB_TARGET) as file:\n",
    "  vocab_lookup = [l.strip() for l in file.readlines()]\n",
    "\" \".join([vocab_lookup[i] for i in predictions_[\"predictions\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
