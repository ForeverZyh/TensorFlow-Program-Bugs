## Download & Setup

To use [seq2seq] you need a working installation of TensorFlow with
Python 2.7 or Python 3.5. Follow the [TensorFlow Getting Started](https://www.tensorflow.org/get_started/os_setup)
for detailed setup instructions. With TensorFlow installed, you can clone this repository:

```bash
git clone https://github.com/dennybritz/seq2seq.git
cd seq2seq
```

To make sure everything works as expect you can run a simple training test:

```bash
./test/train_infer_test.sh
```

If you see a "success" message, you are all set. If you run into setup issues,
please [file a Github issue](https://github.com/dennybritz/seq2seq/issues).


## Using a pre-trained model

Training sequence models can take a long time. For example, training a model to
translate from English to German takes several days on a modern GPU (you can speed this up by using distributed training). If all you want is *use* a model that has already been trained, check out the [models](models.md) page. As an example, let's use a model that has been trained to translate from English to German.

```bash
# Download the trained model
MODEL_DIR=${TMPDIR}/en_de_pretrained
wget TODO

# Create an input file with English sentences to feed to the model
# Note: The input must be tokenized. See the data section for more info.
echo "What is your name ?" > $TMPDIR/test.en

# Call the model inference script
./bin/infer.py \
  --source $TMPDIR/test.en \
  --vocab_source $MODEL_DIR/vocab.en \
  --vocab_target $MODEL_DIR/vocab.de \
  --model AttentionSeq2Seq \
  --model_dir $MODEL_DIR

# You should see German output: TODO
```

## Training your own model

Let's now look at how you can train a model based on your own data. In this section we will train a model that learns to reverse an input sequence. While this task is not very useful in practice, it trains very quickly and is therefore a helpful sanity-check that the end-to-end pipeline is working as intended.

First, let's generate some data:

```bash
DATA_TYPE=reverse ./bin/data/toy.sh
```

To train a new model, we will use the files that are generated by this script, which are described in detail in the table below. To use your own data, you will need to generate corresponding files.

| Argument Name | Description |
| --- | --- |
| `train_source` | The input sequences. A plain text file where each line corresponds to one input example, and tokens/words are separated by spaces. For natural language data you may need to use a tokenizer to generate this. See [tools](tools/) for more details. |
| `train_target` | The targets sequences in the same format as `train_target`. Each line corresponds to the "translation" of the input sequence in the same line in `train_target`. Thus, the number of lines in these two files must be equal. |
| `dev_source` | Same as `train_source` but used as a validation/development set. |
| `dev_target` | Same as `train_target` but used as a validation/development set. |
| `vocab_source` | The vocabulary for the source sequences. A plain text file that contains one word per line. All words that appear in the source text but are not in the vocabulary will be mapped to `UNK` (unknown) tokens. We provide a script to generate a vocabulary file. See [tools](tools/) for more details. |
| `vocab_target` | Same as `vocab_source` but for the target sequences. |


With the above input files you can now train a new model:

```bash
./bin/train.py \
  --train_source $HOME/nmt_data/toy_reverse/train/sources.txt \
  --train_target $HOME/nmt_data/toy_reverse/train/targets.txt \
  --dev_source $HOME/nmt_data/toy_reverse/dev/sources.txt \
  --dev_target $HOME/nmt_data/toy_reverse/dev/targets.txt \
  --vocab_source $HOME/nmt_data/toy_reverse/train/vocab.sources.txt \
  --vocab_target $HOME/nmt_data/toy_reverse/train/vocab.targets.txt \
  --model AttentionSeq2Seq \
  --batch_size 32 \
  --train_epochs 5 \
  --output_dir ${TMPDIR}/nmt_toy_reverse
```

On a CPU, the training may take up to 15 minutes. With the trained model you can now perform inference:

```bash
./bin/infer.py \
  --source $HOME/nmt_data/toy_reverse/test/sources.txt \
  --model_dir ${TMPDIR}/nmt_toy_reverse \
  > ${TMPDIR}/nmt_toy_reverse/predictions.txt

# Evaluate BLEU score using multi-bleu script from MOSES
./bin/tools/multi-bleu.perl $HOME/nmt_data/toy_reverse/test/targets.txt < ${TMPDIR}/nmt_toy_reverse/predictions.txt
```

To learn more about available models, data and tools, please see the corresponding sections of the documentation.


